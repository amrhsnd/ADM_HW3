{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crawler as cr\n",
    "import parser as pr\n",
    "\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.settings import Settings\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of Michelin restaurants\n",
    "\n",
    "You should begin by compiling a list of restaurants to include in your document corpus. Specifically, you will focus on web scraping the [Michelin Restaurants in Italy](https://guide.michelin.com/en/it/restaurants). Your task is to **collect the URL** associated with each restaurant in this list. The output of this step should be a `.txt` file where each line contains a single restaurantâ€™s URL. By the end, you should have approximately 2,037 restaurants on your list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom settings for the url spider\n",
    "custom_settings = Settings({\n",
    "    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',  # Set to recommended value to avoid issues\n",
    "    'LOG_LEVEL': 'ERROR'  # Suppress other logging\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_process = CrawlerProcess(settings=custom_settings) # Create a process for the spider\n",
    "get_url_process.crawl(cr.UrlMichelin) # Add the spider to the process\n",
    "get_url_process.start() # Run the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in file: 1983\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "if os.path.exists('urls.txt'):\n",
    "    # Check the output file and see if the number of lines is correct\n",
    "    lines_in_file = open('urls.txt', 'r').readlines()\n",
    "    number_of_lines = len(lines_in_file)\n",
    "    print(f'Number of lines in file: {number_of_lines}')\n",
    "else:\n",
    "    print('Failure: File not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl Michelin restaurant pages\n",
    "\n",
    "Once you have all the URLs on the list, you should:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After collecting each page, immediately save its `HTML` in a file. This way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the restaurants from page 1, page 2, ... of the Michelin restaurant list.\n",
    "\n",
    "__Tip__: Due to the large number of pages to download, consider using methods that can help shorten the process. If you employed a particular process or approach, kindly describe it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded all pages!\n"
     ]
    }
   ],
   "source": [
    "lines_of_urls = []\n",
    "with open('urls.txt', 'r') as file:\n",
    "    lines_of_urls = file.readlines()\n",
    "\n",
    "original_directory = os.getcwd()\n",
    "os.makedirs('pages', exist_ok=True)\n",
    "os.chdir(os.path.join(original_directory, 'pages'))\n",
    "\n",
    "# Create folders for the HTML files\n",
    "cr.make_folders(100)\n",
    "\n",
    "max_w = os.cpu_count()\n",
    "\n",
    "# Download the HTML files concurrently\n",
    "with ThreadPoolExecutor(max_workers=max_w) as executor:\n",
    "    download_futures = []\n",
    "    for line in lines_of_urls:\n",
    "        # Split the line into URL and page number\n",
    "        page_num = int(line.split(\"|\")[1])\n",
    "        url = line.split(\"|\")[0].strip()\n",
    "                \n",
    "        # Submit download task to the executor\n",
    "        download_futures.append(executor.submit(cr.HTML_downloader, url, page_num))\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    for future in as_completed(download_futures):\n",
    "        pass\n",
    "\n",
    "# Notify completion\n",
    "print(\"Downloaded all pages!\")\n",
    "\n",
    "# Return to the original directory\n",
    "os.chdir(original_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count: 1983\n"
     ]
    }
   ],
   "source": [
    "# Check if the files exist and are 1983\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "dir_path = os.path.join(current_dir, 'pages')\n",
    "os.chdir(dir_path)\n",
    "count = 0\n",
    "\n",
    "for i in range(1,101):\n",
    "    folder = f'page_{i}'\n",
    "    for path in os.listdir(folder):\n",
    "        if os.path.isfile(os.path.join(folder, path)):\n",
    "            count += 1\n",
    "\n",
    "os.chdir(current_dir)\n",
    "\n",
    "print('File count:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "At this point, you should have all the HTML documents about the restaurant of interest, and you can start to extract specific information. The list of the information we desire for each restaurant and their format is as follows:\n",
    "\n",
    "1. **Restaurant Name** (to save as `restaurantName`): string;\n",
    "2. **Address** (to save as `address`): string;\n",
    "3. **City** (to save as `city`): string;\n",
    "4. **Postal Code** (to save as `postalCode`): string;\n",
    "5. **Country** (to save as `country`): string;\n",
    "6. **Price Range** (to save as `priceRange`): string;\n",
    "7. **Cuisine Type** (to save as `cuisineType`): string;\n",
    "8. **Description** (to save as `description`): string;\n",
    "9. **Facilities and Services** (to save as `facilitiesServices`): list of strings;\n",
    "10. **Accepted Credit Cards** (to save as `creditCards`): list of strings;\n",
    "11. **Phone Number** (to save as `phoneNumber`): string;\n",
    "12. **URL to the Restaurant Page** (to save as `website`): string.\n",
    "\n",
    "For each restaurant, you create a `restaurant_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "restaurantName \\t address \\t  ... \\t url\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all data!\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "os.makedirs('data_tsv', exist_ok=True)\n",
    "dest_dir = os.path.join(current_dir, 'data_tsv')\n",
    "\n",
    "keys = ['index', 'restaurantName', 'address', 'city', 'postalCode', 'country', 'priceRange', 'cuisineType', 'description', 'creditCards', 'facilitiesServices', 'phoneNumber', 'website']\n",
    "\n",
    "max_w = os.cpu_count()\n",
    "\n",
    "# Download the data from HTML files concurrently\n",
    "with ThreadPoolExecutor(max_workers=max_w) as executor:\n",
    "    extractor_future = []\n",
    "    for i in range(1,101):\n",
    "        start_dir = os.path.join('pages', f'page_{i}')\n",
    "        start_index = (i-1)*20 \n",
    "        extractor_future.append(executor.submit(pr.tsv_extractor, start_dir, dest_dir, start_index, keys))\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    for future in as_completed(extractor_future):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "# Notify completion\n",
    "print(\"Extracted all data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count: 1983\n"
     ]
    }
   ],
   "source": [
    "# Check if the files exist and are 1983\n",
    "current_dir = os.getcwd()\n",
    "dir_path = os.path.join(current_dir, 'data_tsv')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "        if os.path.isfile(os.path.join('data_tsv', path)):\n",
    "            count += 1\n",
    "\n",
    "print('File count:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified all data!\n"
     ]
    }
   ],
   "source": [
    "# Unify all data into one file csv\n",
    "\n",
    "# Directory containing the TSV files\n",
    "current_dir = os.getcwd()\n",
    "dir_path = os.path.join(current_dir, 'data_tsv')\n",
    "\n",
    "# List all TSV files in the directory\n",
    "tsv_files = [f for f in os.listdir(dir_path) if f.endswith('.tsv')]\n",
    "\n",
    "# Load all TSV files into a list of dataframes\n",
    "dfs = [pd.read_csv(os.path.join(dir_path, file), sep='\\t') for file in tsv_files]\n",
    "\n",
    "# Unite all dataframes into one\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "merged_df.sort_values(by=['index'], inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a TSV file\n",
    "merged_df.to_csv('dataset.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Notify completion\n",
    "print(\"Unified all data!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
